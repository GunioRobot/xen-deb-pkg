--- a/tools/libxc/xc_domain.c
+++ b/tools/libxc/xc_domain.c
@@ -25,6 +25,15 @@
 #include <xen/memory.h>
 #include <xen/hvm/hvm_op.h>
 
+int xc_interface_restrict(xc_interface *xch, domid_t dom)
+{
+    if (xch->ops->u.privcmd.restrict_to)
+        return xch->ops->u.privcmd.restrict_to(xch, xch->ops_handle, dom);
+
+    /* no hook implies the backend doesn't need/want this */
+    return 0;
+}
+
 int xc_domain_create(xc_interface *xch,
                      uint32_t ssidref,
                      xen_domain_handle_t handle,
@@ -84,32 +93,31 @@
                        uint32_t domid,
                        int reason)
 {
-    int ret = -1;
     DECLARE_HYPERCALL;
     DECLARE_HYPERCALL_BUFFER(sched_remote_shutdown_t, arg);
+    int rc;
 
     arg = xc_hypercall_buffer_alloc(xch, arg, sizeof(*arg));
     if ( arg == NULL )
-    {
-        PERROR("Could not allocate memory for xc_domain_shutdown hypercall");
-        goto out1;
-    }
+        return -1;
 
-    hypercall.op     = __HYPERVISOR_sched_op;
-    hypercall.arg[0] = (unsigned long)SCHEDOP_remote_shutdown;
-    hypercall.arg[1] = HYPERCALL_BUFFER_AS_ARG(arg);
     arg->domain_id = domid;
     arg->reason = reason;
 
-    ret = do_xen_hypercall(xch, &hypercall);
+    rc = do_xen_arch_ioctl(xch, IOCTL_PRIVCMD_SHUTDOWN,
+                           HYPERCALL_BUFFER_AS_ARG(arg));
+    if (rc < 0 && errno == EINVAL) {
+        hypercall.op     = __HYPERVISOR_sched_op;
+        hypercall.arg[0] = (unsigned long)SCHEDOP_remote_shutdown;
+        hypercall.arg[1] = HYPERCALL_BUFFER_AS_ARG(arg);
 
-    xc_hypercall_buffer_free(xch, arg);
+        rc = do_xen_hypercall(xch, &hypercall);
+    }
 
- out1:
-    return ret;
+    xc_hypercall_buffer_free(xch, arg);
+    return rc;
 }
 
-
 int xc_vcpu_setaffinity(xc_interface *xch,
                         uint32_t domid,
                         int vcpu,
@@ -617,6 +625,17 @@
 }
 
 
+int xc_domain_set_cores_per_socket(xc_interface *xch,
+                                   uint32_t domid,
+                                   uint32_t cores_per_socket)
+{
+    DECLARE_DOMCTL;
+    domctl.cmd = XEN_DOMCTL_setcorespersocket;
+    domctl.domain = (domid_t)domid;
+    domctl.u.setcorespersocket.cores_per_socket = cores_per_socket;
+    return do_domctl(xch, &domctl);
+}
+
 int xc_domain_maximum_gpfn(xc_interface *xch, domid_t domid)
 {
     return do_memory_op(xch, XENMEM_maximum_gpfn, &domid, sizeof(domid));
@@ -961,6 +980,24 @@
     return rc;
 }
 
+int xc_get_runstate_info(xc_interface *xch, uint32_t domid, xc_runstate_info_t *info)
+{
+    int ret = -EBADF;
+    DECLARE_DOMCTL;
+
+    domctl.cmd = XEN_DOMCTL_get_runstate_info;
+    domctl.domain = domid;
+
+    ret = do_domctl(xch, &domctl);
+    if (ret < 0) {
+        ERROR("get runstate info");
+        return ret;
+    }
+
+    memcpy(info, &domctl.u.domain_runstate, sizeof(*info));
+    return ret;
+}
+
 int xc_domain_ioport_permission(xc_interface *xch,
                                 uint32_t domid,
                                 uint32_t first_port,
@@ -1078,44 +1115,30 @@
 
 int xc_set_hvm_param(xc_interface *handle, domid_t dom, int param, unsigned long value)
 {
-    DECLARE_HYPERCALL;
-    DECLARE_HYPERCALL_BUFFER(xen_hvm_param_t, arg);
-    int rc;
+    xen_hvm_param_t arg;
 
-    arg = xc_hypercall_buffer_alloc(handle, arg, sizeof(*arg));
-    if ( arg == NULL )
-        return -1;
-
-    hypercall.op     = __HYPERVISOR_hvm_op;
-    hypercall.arg[0] = HVMOP_set_param;
-    hypercall.arg[1] = HYPERCALL_BUFFER_AS_ARG(arg);
-    arg->domid = dom;
-    arg->index = param;
-    arg->value = value;
-    rc = do_xen_hypercall(handle, &hypercall);
-    xc_hypercall_buffer_free(handle, arg);
-    return rc;
+    arg.domid = dom;
+    arg.index = param;
+    arg.value = value;
+    
+    return do_hvm_op(handle, HVMOP_set_param, &arg);
 }
 
 int xc_get_hvm_param(xc_interface *handle, domid_t dom, int param, unsigned long *value)
 {
-    DECLARE_HYPERCALL;
-    DECLARE_HYPERCALL_BUFFER(xen_hvm_param_t, arg);
+    xen_hvm_param_t arg;
     int rc;
 
-    arg = xc_hypercall_buffer_alloc(handle, arg, sizeof(*arg));
-    if ( arg == NULL )
-        return -1;
+    arg.domid = dom;
+    arg.index = param;
+    arg.value = 0;
+    
+    rc = do_hvm_op(handle, HVMOP_get_param, &arg);
+    if (rc)
+        return rc;
 
-    hypercall.op     = __HYPERVISOR_hvm_op;
-    hypercall.arg[0] = HVMOP_get_param;
-    hypercall.arg[1] = HYPERCALL_BUFFER_AS_ARG(arg);
-    arg->domid = dom;
-    arg->index = param;
-    rc = do_xen_hypercall(handle, &hypercall);
-    *value = arg->value;
-    xc_hypercall_buffer_free(handle, arg);
-    return rc;
+    *value = arg.value;
+    return 0;
 }
 
 int xc_domain_setdebugging(xc_interface *xch,
@@ -1482,6 +1505,68 @@
     return do_domctl(xch, &domctl);
 }
 
+int xc_domain_send_s3resume(xc_interface *xch, unsigned int domid)
+{
+    return xc_set_hvm_param(xch, domid, HVM_PARAM_ACPI_S_STATE, 0);
+}
+
+int xc_domain_set_timer_mode(xc_interface *xch, unsigned int domid, int mode)
+{
+    return xc_set_hvm_param(xch, domid,
+                            HVM_PARAM_TIMER_MODE, (unsigned long) mode);
+}
+
+int xc_domain_set_hpet(xc_interface *xch, unsigned int domid, int hpet)
+{
+    return xc_set_hvm_param(xch, domid, HVM_PARAM_HPET_ENABLED, (unsigned long) hpet);
+}
+
+int xc_domain_set_vpt_align(xc_interface *xch, unsigned int domid, int vpt_align)
+{
+    return xc_set_hvm_param(xch, domid, HVM_PARAM_HPET_ENABLED, (unsigned long) vpt_align);
+}
+
+int xc_domain_get_acpi_s_state(xc_interface *xch, unsigned int domid)
+{
+    int ret;
+    unsigned long value;
+
+    ret = xc_get_hvm_param(xch, domid, HVM_PARAM_ACPI_S_STATE, &value);
+    if (ret != 0)
+        ERROR("get acpi s-state");
+    return value;
+}
+
+int xc_domain_trigger_power(xc_interface *xch, unsigned int domid)
+{
+    int ret;
+    DECLARE_DOMCTL;
+
+    domctl.cmd = XEN_DOMCTL_sendtrigger;
+    domctl.domain = domid;
+    domctl.u.sendtrigger.trigger = XEN_DOMCTL_SENDTRIGGER_POWER;
+
+    ret = do_domctl(xch, &domctl);
+    if (ret != 0)
+        ERROR("power button failed");
+    return ret;
+}
+
+int xc_domain_trigger_sleep(xc_interface *xch, unsigned int domid)
+{
+    int ret;
+    DECLARE_DOMCTL;
+
+    domctl.cmd = XEN_DOMCTL_sendtrigger;
+    domctl.domain = domid;
+    domctl.u.sendtrigger.trigger = XEN_DOMCTL_SENDTRIGGER_SLEEP;
+
+    ret = do_domctl(xch, &domctl);
+    if (ret != 0)
+        ERROR("sleep button failed");
+    return ret;
+}
+
 /*
  * Local variables:
  * mode: C
--- a/tools/libxc/xc_misc.c
+++ b/tools/libxc/xc_misc.c
@@ -323,38 +323,46 @@
 }
 
 
-int xc_hvm_set_pci_intx_level(
-    xc_interface *xch, domid_t dom,
-    uint8_t domain, uint8_t bus, uint8_t device, uint8_t intx,
-    unsigned int level)
+int xc_get_boot_cpufeatures(xc_interface *xch,
+                            uint32_t *base_ecx, uint32_t *base_edx,
+                            uint32_t *ext_ecx, uint32_t *ext_edx,
+                            uint32_t *masked_base_ecx, 
+                            uint32_t *masked_base_edx,
+                            uint32_t *masked_ext_ecx, 
+                            uint32_t *masked_ext_edx)
 {
-    DECLARE_HYPERCALL;
-    DECLARE_HYPERCALL_BUFFER(struct xen_hvm_set_pci_intx_level, arg);
+    xen_platform_op_t pm = {0};
     int rc;
 
-    arg = xc_hypercall_buffer_alloc(xch, arg, sizeof(*arg));
-    if ( arg == NULL )
-    {
-        PERROR("Could not allocate memory for xc_hvm_set_pci_intx_level hypercall");
-        return -1;
-    }
-
-    hypercall.op     = __HYPERVISOR_hvm_op;
-    hypercall.arg[0] = HVMOP_set_pci_intx_level;
-    hypercall.arg[1] = HYPERCALL_BUFFER_AS_ARG(arg);
+    pm.cmd = XENPF_get_cpu_features;
+    rc = do_platform(xch, &pm);
 
-    arg->domid  = dom;
-    arg->domain = domain;
-    arg->bus    = bus;
-    arg->device = device;
-    arg->intx   = intx;
-    arg->level  = level;
+    *base_ecx = pm.u.cpu_features.base_ecx;
+    *base_edx = pm.u.cpu_features.base_edx;
+    *ext_ecx = pm.u.cpu_features.ext_ecx;
+    *ext_edx = pm.u.cpu_features.ext_edx;
+    *masked_base_ecx = pm.u.cpu_features.masked_base_ecx;
+    *masked_base_edx = pm.u.cpu_features.masked_base_edx;
+    *masked_ext_ecx = pm.u.cpu_features.masked_ext_ecx;
+    *masked_ext_edx = pm.u.cpu_features.masked_ext_edx;
+    return rc;
+}
 
-    rc = do_xen_hypercall(xch, &hypercall);
+int xc_hvm_set_pci_intx_level(
+    xc_interface *xch, domid_t dom,
+    uint8_t domain, uint8_t bus, uint8_t device, uint8_t intx,
+    unsigned int level)
+{
+    struct xen_hvm_set_pci_intx_level arg;
 
-    xc_hypercall_buffer_free(xch, arg);
+    arg.domid  = dom;
+    arg.domain = domain;
+    arg.bus    = bus;
+    arg.device = device;
+    arg.intx   = intx;
+    arg.level  = level;
 
-    return rc;
+    return do_hvm_op(xch, HVMOP_set_pci_intx_level, &arg);
 }
 
 int xc_hvm_set_isa_irq_level(
@@ -362,59 +370,25 @@
     uint8_t isa_irq,
     unsigned int level)
 {
-    DECLARE_HYPERCALL;
-    DECLARE_HYPERCALL_BUFFER(struct xen_hvm_set_isa_irq_level, arg);
-    int rc;
-
-    arg = xc_hypercall_buffer_alloc(xch, arg, sizeof(*arg));
-    if ( arg == NULL )
-    {
-        PERROR("Could not allocate memory for xc_hvm_set_isa_irq_level hypercall");
-        return -1;
-    }
+    struct xen_hvm_set_isa_irq_level arg;
 
-    hypercall.op     = __HYPERVISOR_hvm_op;
-    hypercall.arg[0] = HVMOP_set_isa_irq_level;
-    hypercall.arg[1] = HYPERCALL_BUFFER_AS_ARG(arg);
+    arg.domid   = dom;
+    arg.isa_irq = isa_irq;
+    arg.level   = level;
 
-    arg->domid   = dom;
-    arg->isa_irq = isa_irq;
-    arg->level   = level;
-
-    rc = do_xen_hypercall(xch, &hypercall);
-
-    xc_hypercall_buffer_free(xch, arg);
-
-    return rc;
+    return do_hvm_op(xch, HVMOP_set_isa_irq_level, &arg);
 }
 
 int xc_hvm_set_pci_link_route(
     xc_interface *xch, domid_t dom, uint8_t link, uint8_t isa_irq)
 {
-    DECLARE_HYPERCALL;
-    DECLARE_HYPERCALL_BUFFER(struct xen_hvm_set_pci_link_route, arg);
-    int rc;
-
-    arg = xc_hypercall_buffer_alloc(xch, arg, sizeof(*arg));
-    if ( arg == NULL )
-    {
-        PERROR("Could not allocate memory for xc_hvm_set_pci_link_route hypercall");
-        return -1;
-    }
-
-    hypercall.op     = __HYPERVISOR_hvm_op;
-    hypercall.arg[0] = HVMOP_set_pci_link_route;
-    hypercall.arg[1] = HYPERCALL_BUFFER_AS_ARG(arg);
-
-    arg->domid   = dom;
-    arg->link    = link;
-    arg->isa_irq = isa_irq;
-
-    rc = do_xen_hypercall(xch, &hypercall);
+    struct xen_hvm_set_pci_link_route arg;
 
-    xc_hypercall_buffer_free(xch, arg);
+    arg.domid   = dom;
+    arg.link    = link;
+    arg.isa_irq = isa_irq;
 
-    return rc;
+    return do_hvm_op(xch, HVMOP_set_pci_link_route, &arg);
 }
 
 int xc_hvm_inject_msi(
@@ -451,32 +425,22 @@
     uint64_t first_pfn, uint64_t nr,
     unsigned long *dirty_bitmap)
 {
-    DECLARE_HYPERCALL;
+    struct xen_hvm_track_dirty_vram arg;
     DECLARE_HYPERCALL_BOUNCE(dirty_bitmap, (nr+7) / 8, XC_HYPERCALL_BUFFER_BOUNCE_OUT);
-    DECLARE_HYPERCALL_BUFFER(struct xen_hvm_track_dirty_vram, arg);
     int rc;
 
-    arg = xc_hypercall_buffer_alloc(xch, arg, sizeof(*arg));
-    if ( arg == NULL || xc_hypercall_bounce_pre(xch, dirty_bitmap) )
+    if ( xc_hypercall_bounce_pre(xch, dirty_bitmap) )
     {
         PERROR("Could not bounce memory for xc_hvm_track_dirty_vram hypercall");
-        rc = -1;
-        goto out;
+        return -1;
     }
 
-    hypercall.op     = __HYPERVISOR_hvm_op;
-    hypercall.arg[0] = HVMOP_track_dirty_vram;
-    hypercall.arg[1] = HYPERCALL_BUFFER_AS_ARG(arg);
-
-    arg->domid     = dom;
-    arg->first_pfn = first_pfn;
-    arg->nr        = nr;
-    set_xen_guest_handle(arg->dirty_bitmap, dirty_bitmap);
-
-    rc = do_xen_hypercall(xch, &hypercall);
+    arg.domid     = dom;
+    arg.first_pfn = first_pfn;
+    arg.nr        = nr;
+    set_xen_guest_handle(arg.dirty_bitmap, dirty_bitmap);
 
-out:
-    xc_hypercall_buffer_free(xch, arg);
+    rc = do_hvm_op(xch, HVMOP_track_dirty_vram, &arg);
     xc_hypercall_bounce_post(xch, dirty_bitmap);
     return rc;
 }
@@ -484,60 +448,26 @@
 int xc_hvm_modified_memory(
     xc_interface *xch, domid_t dom, uint64_t first_pfn, uint64_t nr)
 {
-    DECLARE_HYPERCALL;
-    DECLARE_HYPERCALL_BUFFER(struct xen_hvm_modified_memory, arg);
-    int rc;
-
-    arg = xc_hypercall_buffer_alloc(xch, arg, sizeof(*arg));
-    if ( arg == NULL )
-    {
-        PERROR("Could not allocate memory for xc_hvm_modified_memory hypercall");
-        return -1;
-    }
-
-    hypercall.op     = __HYPERVISOR_hvm_op;
-    hypercall.arg[0] = HVMOP_modified_memory;
-    hypercall.arg[1] = HYPERCALL_BUFFER_AS_ARG(arg);
-
-    arg->domid     = dom;
-    arg->first_pfn = first_pfn;
-    arg->nr        = nr;
-
-    rc = do_xen_hypercall(xch, &hypercall);
+    struct xen_hvm_modified_memory arg;
 
-    xc_hypercall_buffer_free(xch, arg);
+    arg.domid     = dom;
+    arg.first_pfn = first_pfn;
+    arg.nr        = nr;
 
-    return rc;
+    return do_hvm_op(xch, HVMOP_modified_memory, &arg);
 }
 
 int xc_hvm_set_mem_type(
     xc_interface *xch, domid_t dom, hvmmem_type_t mem_type, uint64_t first_pfn, uint64_t nr)
 {
-    DECLARE_HYPERCALL;
-    DECLARE_HYPERCALL_BUFFER(struct xen_hvm_set_mem_type, arg);
-    int rc;
-
-    arg = xc_hypercall_buffer_alloc(xch, arg, sizeof(*arg));
-    if ( arg == NULL )
-    {
-        PERROR("Could not allocate memory for xc_hvm_set_mem_type hypercall");
-        return -1;
-    }
-
-    arg->domid        = dom;
-    arg->hvmmem_type  = mem_type;
-    arg->first_pfn    = first_pfn;
-    arg->nr           = nr;
-
-    hypercall.op     = __HYPERVISOR_hvm_op;
-    hypercall.arg[0] = HVMOP_set_mem_type;
-    hypercall.arg[1] = HYPERCALL_BUFFER_AS_ARG(arg);
-
-    rc = do_xen_hypercall(xch, &hypercall);
+    struct xen_hvm_set_mem_type arg;
 
-    xc_hypercall_buffer_free(xch, arg);
+    arg.domid        = dom;
+    arg.hvmmem_type  = mem_type;
+    arg.first_pfn    = first_pfn;
+    arg.nr           = nr;
 
-    return rc;
+    return do_hvm_op(xch, HVMOP_set_mem_type, &arg);
 }
 
 int xc_hvm_set_mem_access(
@@ -633,6 +563,26 @@
     return rc;
 }
 
+int xc_hvm_check_pvdriver(xc_interface *xch, unsigned int domid)
+{
+    int ret;
+    unsigned long irq = 0;
+    xc_domaininfo_t info;
+
+    ret = xc_domain_getinfolist(xch, domid, 1, &info);
+    if (ret != 1) {
+        PERROR("domain getinfo failed");
+        return -1;
+    }
+
+    if ((!info.flags) & XEN_DOMINF_hvm_guest) {
+        ERROR("domain is not hvm");
+        return -1;
+    }
+    xc_get_hvm_param(xch, domid, HVM_PARAM_CALLBACK_IRQ, &irq);
+    return irq;
+}
+
 /*
  * Local variables:
  * mode: C
--- a/tools/libxc/xenctrl.h
+++ b/tools/libxc/xenctrl.h
@@ -38,6 +38,7 @@
 #include <xen/domctl.h>
 #include <xen/physdev.h>
 #include <xen/sysctl.h>
+#include <xen/platform.h>
 #include <xen/version.h>
 #include <xen/event_channel.h>
 #include <xen/sched.h>
@@ -177,6 +178,14 @@
  */
 int xc_interface_is_fake(void);
 
+ /**
+  * Restrict a handle to the event channel driver so that it only works
+  * for one domain.
+  *
+  * @return zero on success
+  */
+int xc_evtchn_restrict(xc_evtchn *xce, domid_t domid);
+
 /*
  * HYPERCALL SAFE MEMORY BUFFER
  *
@@ -323,6 +332,18 @@
 /* allocate a cpumap */
 xc_cpumap_t xc_cpumap_alloc(xc_interface *xch);
 
+/**
+ * This function restricts an open hypervisor interface so that it can
+ * only be applied to a particular domain.  It cannot subsequently be
+ * unrestricted.
+ *
+ *
+ * @parm xc_handle a handle to an open hypervisor interface
+ * @parm domid the domain to restrict to
+ * @return 0 on success, -1 otherwise.
+ */
+int xc_interface_restrict(xc_interface *xch, domid_t domid);
+
 /*
  * DOMAIN DEBUGGING FUNCTIONS
  */
@@ -363,6 +384,7 @@
 } xc_dominfo_t;
 
 typedef xen_domctl_getdomaininfo_t xc_domaininfo_t;
+typedef xen_domctl_runstate_info_t xc_runstate_info_t;
 
 typedef union 
 {
@@ -608,6 +630,15 @@
                              uint8_t *hvm_ctxt,
                              uint32_t size);
 
+int xc_domain_send_s3resume(xc_interface *xch, unsigned int domid);
+int xc_domain_set_timer_mode(xc_interface *xch, unsigned int domid, int mode);
+int xc_domain_set_hpet(xc_interface *xch, unsigned int domid, int hpet);
+int xc_domain_set_vpt_align(xc_interface *xch, unsigned int domid, int vpt_align);
+int xc_domain_get_acpi_s_state(xc_interface *xch, unsigned int domid);
+
+int xc_domain_trigger_power(xc_interface *xch, unsigned int domid);
+int xc_domain_trigger_sleep(xc_interface *xch, unsigned int domid);
+
 /**
  * This function returns information about the execution context of a
  * particular vcpu of a domain.
@@ -1014,6 +1045,10 @@
 
 int xc_domain_disable_migrate(xc_interface *xch, uint32_t domid);
 
+int xc_domain_set_cores_per_socket(xc_interface *xch,
+				   uint32_t domid,
+				   uint32_t cores_per_socket);
+ 
 int xc_domain_maximum_gpfn(xc_interface *xch, domid_t domid);
 
 int xc_domain_increase_reservation(xc_interface *xch,
@@ -1084,6 +1119,10 @@
                              uint64_t *pod_cache_pages,
                              uint64_t *pod_entries);
 
+int xc_get_runstate_info(xc_interface *xch,
+			 uint32_t domid,
+			 xc_runstate_info_t *info);
+
 int xc_domain_ioport_permission(xc_interface *xch,
                                 uint32_t domid,
                                 uint32_t first_port,
@@ -1535,6 +1574,8 @@
     xc_interface *xch, domid_t dom, int vcpu, uint32_t trap, uint32_t error_code, 
     uint64_t cr2);
 
+int xc_hvm_check_pvdriver(xc_interface *xch, unsigned int domid);
+
 /*
  *  LOGGING AND ERROR REPORTING
  */
@@ -1906,4 +1947,15 @@
                         int verbose);
 /* Useful for callers who also use libelf. */
 
+
+/* Get the CPUID feature lists before and after any hardware masks 
+ * were applied.  Returns the ANDed aggregate of all online CPUs. */
+int xc_get_boot_cpufeatures(xc_interface *xc_handle, 
+                            uint32_t *base_ecx, uint32_t *base_edx,
+                            uint32_t *ext_ecx, uint32_t *ext_edx,
+                            uint32_t *masked_base_ecx, 
+                            uint32_t *masked_base_edx,
+                            uint32_t *masked_ext_ecx, 
+                            uint32_t *masked_ext_edx);
+
 #endif /* XENCTRL_H */
--- a/tools/include/xen-sys/Linux/evtchn.h
+++ b/tools/include/xen-sys/Linux/evtchn.h
@@ -43,6 +43,8 @@
 	unsigned int virq;
 };
 
+#include <xen/xen.h>
+
 /*
  * Bind a fresh port to remote <@remote_domain, @remote_port>.
  * Return allocated port.
@@ -85,4 +87,17 @@
 #define IOCTL_EVTCHN_RESET				\
 	_IOC(_IOC_NONE, 'E', 5, 0)
 
+/* Restruct this file descriptor so that it can only be applied to a
+ * nominated domain.  Once a file descriptor has been restricted it
+ * cannot be de-restricted, and must be closed and re-openned.  Event
+ * channels which were bound before restricting remain bound
+ * afterwards, and can be notified as usual.
+ */
+#define IOCTL_EVTCHN_RESTRICT_DOMID                    \
+       _IOC(_IOC_NONE, 'E', 100, sizeof(struct ioctl_evtchn_restrict_domid))
+struct ioctl_evtchn_restrict_domid {
+       domid_t domid;
+};
+
+
 #endif /* __LINUX_PUBLIC_EVTCHN_H__ */
--- a/tools/include/xen-sys/Linux/privcmd.h
+++ b/tools/include/xen-sys/Linux/privcmd.h
@@ -35,6 +35,14 @@
 
 #include <linux/types.h>
 
+#ifdef __KERNEL__
+#include <xen/interface/hvm/hvm_op.h>
+#include <xen/interface/memory.h>
+#else
+#include <xen/hvm/hvm_op.h>
+#include <xen/memory.h>
+#endif
+
 #ifndef __user
 #define __user
 #endif
@@ -72,6 +80,37 @@
 	int __user *err;  /* array of error codes */
 } privcmd_mmapbatch_v2_t;
 
+typedef struct privcmd_restrict_domid {
+       domid_t domid;
+} privcmd_restrict_domid_t;
+
+typedef struct privcmd_memop {
+        unsigned cmd;
+        union {
+                xen_memory_reservation_t reservation;
+                xen_memory_exchange_t exchange;
+                domid_t domid;
+                xen_add_to_physmap_t add_to_physmap;
+                xen_foreign_memory_map_t foreign_memory_map;
+                xen_machphys_mfn_list_t machphys_mfn_list;
+                xen_machphys_mapping_t machphys_mapping;
+                xen_memory_map_t memory_map;
+        } u;
+} privcmd_memop_t;
+
+typedef struct privcmd_hvmop {
+        unsigned cmd;
+        union {
+                xen_hvm_param_t param;
+                xen_hvm_set_pci_intx_level_t set_pci_intx_level;
+                xen_hvm_set_isa_irq_level_t set_isa_irq_level;
+                xen_hvm_set_pci_link_route_t set_pci_link_route;
+                xen_hvm_modified_memory_t modified_memory;
+                xen_hvm_set_mem_type_t set_mem_type;
+                xen_hvm_track_dirty_vram_t track_dirty_vram;
+        } u;
+} privcmd_hvmop_t;
+
 /*
  * @cmd: IOCTL_PRIVCMD_HYPERCALL
  * @arg: &privcmd_hypercall_t
@@ -85,5 +124,17 @@
 	_IOC(_IOC_NONE, 'P', 3, sizeof(privcmd_mmapbatch_t))
 #define IOCTL_PRIVCMD_MMAPBATCH_V2				\
 	_IOC(_IOC_NONE, 'P', 4, sizeof(privcmd_mmapbatch_v2_t))
+#define IOCTL_PRIVCMD_RESTRICT_DOMID                           \
+       _IOC(_IOC_NONE, 'P', 100, sizeof(privcmd_restrict_domid_t))
+#define IOCTL_PRIVCMD_DOMCTL                           \
+       _IOC(_IOC_NONE, 'P', 101, sizeof(xen_domctl_t))
+#define IOCTL_PRIVCMD_HVMOP                                    \
+       _IOC(_IOC_NONE, 'P', 102, sizeof(privcmd_hvmop_t))
+#define IOCTL_PRIVCMD_SHUTDOWN                                 \
+       _IOC(_IOC_NONE, 'P', 103, sizeof(sched_remote_shutdown_t))
+#define IOCTL_PRIVCMD_MEMOP                                    \
+       _IOC(_IOC_NONE, 'P', 104, sizeof(privcmd_memop_t))
+
+#define ARCH_IOCTL(d, r, a) ioctl(d, r, a)
 
 #endif /* __LINUX_PUBLIC_PRIVCMD_H__ */
--- a/tools/include/xen-sys/MiniOS/privcmd.h
+++ b/tools/include/xen-sys/MiniOS/privcmd.h
@@ -22,6 +22,7 @@
  */
 
 #include <sys/types.h>
+#include <errno.h>
 
 typedef struct privcmd_hypercall
 {
@@ -33,4 +34,6 @@
 	uint64_t mfn;
 } privcmd_mmap_entry_t; 
 
+#define ARCH_IOCTL(d, r, a) (errno=EINVAL, -1)
+
 #endif /* __MINIOS_PUBLIC_PRIVCMD_H__ */
--- a/tools/include/xen-sys/NetBSD/privcmd.h
+++ b/tools/include/xen-sys/NetBSD/privcmd.h
@@ -86,6 +86,8 @@
 #define IOCTL_PRIVCMD_GET_MACH2PHYS_START_MFN \
     _IOR('P', 4, unsigned long)
 
+#define ARCH_IOCTL(d, r, a) ioctl(d, r, a)
+
 /*
  * @cmd: IOCTL_PRIVCMD_INITDOMAIN_EVTCHN
  * @arg: n/a
--- a/tools/include/xen-sys/SunOS/privcmd.h
+++ b/tools/include/xen-sys/SunOS/privcmd.h
@@ -53,6 +53,7 @@
 #define	IOCTL_PRIVCMD_HYPERCALL		(__PRIVCMD_IOC|0)
 #define	IOCTL_PRIVCMD_MMAP		(__PRIVCMD_IOC|1)
 #define	IOCTL_PRIVCMD_MMAPBATCH		(__PRIVCMD_IOC|2)
+#define ARCH_IOCTL(d, r, a) ioctl(d, r, a)
 
 typedef struct __privcmd_hypercall {
 	unsigned long op;
--- a/tools/libxc/xc_dom_core.c
+++ b/tools/libxc/xc_dom_core.c
@@ -820,6 +820,12 @@
     return -1;
 }
 
+/* Accessors */
+const char *xc_dom_get_native_protocol(struct xc_dom_image *dom)
+{
+    return dom->arch_hooks ? dom->arch_hooks->native_protocol : "";
+}
+
 /*
  * Local variables:
  * mode: C
--- a/tools/libxc/xc_evtchn.c
+++ b/tools/libxc/xc_evtchn.c
@@ -65,6 +65,15 @@
     return rc;
 }
 
+int xc_evtchn_restrict(xc_evtchn *xce, domid_t dom)
+{
+    if (xce->ops->u.evtchn.restrict_to)
+        return xce->ops->u.evtchn.restrict_to(xce, xce->ops_handle, dom);
+    
+    /* no hook implies the backend doesn't need/want this */
+    return 0;
+}
+
 int xc_evtchn_reset(xc_interface *xch,
                     uint32_t dom)
 {
--- a/tools/libxc/xc_linux_osdep.c
+++ b/tools/libxc/xc_linux_osdep.c
@@ -43,6 +43,15 @@
 #define PERROR(_m, _a...) xc_osdep_log(xch,XTL_ERROR,XC_INTERNAL_ERROR,_m \
                   " (%d = %s)", ## _a , errno, xc_strerror(xch, errno))
 
+static int intf_restrict(xc_interface *xch, xc_osdep_handle h, domid_t domid)
+{
+    privcmd_restrict_domid_t prd;
+    int fd = (int)h;
+
+    prd.domid = domid;
+    return ioctl(fd, IOCTL_PRIVCMD_RESTRICT_DOMID, &prd);
+}
+
 static xc_osdep_handle linux_privcmd_open(xc_interface *xch)
 {
     int flags, saved_errno;
@@ -117,6 +126,12 @@
     return ioctl(fd, IOCTL_PRIVCMD_HYPERCALL, hypercall);
 }
 
+static int linux_arch_ioctl(xc_interface *xch, xc_osdep_handle h, int cmd, long int arg)
+{
+    int fd = (int)h;
+    return ioctl(fd, cmd, arg);
+}
+
 static int xc_map_foreign_batch_single(int fd, uint32_t dom,
                                        xen_pfn_t *mfn, unsigned long addr)
 {
@@ -373,16 +388,27 @@
         .free_hypercall_buffer = &linux_privcmd_free_hypercall_buffer,
 
         .hypercall = &linux_privcmd_hypercall,
+        .arch_ioctl = &linux_arch_ioctl,
 
         .map_foreign_batch = &linux_privcmd_map_foreign_batch,
         .map_foreign_bulk = &linux_privcmd_map_foreign_bulk,
         .map_foreign_range = &linux_privcmd_map_foreign_range,
         .map_foreign_ranges = &linux_privcmd_map_foreign_ranges,
+        .restrict_to = &intf_restrict,
     },
 };
 
 #define DEVXEN "/dev/xen/"
 
+static int evtchn_restrict(xc_evtchn *xce, xc_osdep_handle h, domid_t domid)
+{
+    struct ioctl_evtchn_restrict_domid ierd;
+    int fd = (int)h;
+
+    ierd.domid = domid;
+    return ioctl(fd, IOCTL_EVTCHN_RESTRICT_DOMID, &ierd);
+}
+
 static xc_osdep_handle linux_evtchn_open(xc_evtchn *xce)
 {
     int fd = open(DEVXEN "evtchn", O_RDWR);
@@ -491,6 +517,7 @@
         .unbind = &linux_evtchn_unbind,
         .pending = &linux_evtchn_pending,
         .unmask = &linux_evtchn_unmask,
+        .restrict_to = &evtchn_restrict,
     },
 };
 
--- a/tools/libxc/xc_minios.c
+++ b/tools/libxc/xc_minios.c
@@ -103,6 +103,56 @@
     return call.result;
 }
 
+int xc_memory_op_new(int xc_handle, int cmd, void *arg)
+{
+    errno = EINVAL;
+    return -1;
+}
+
+int do_hvm_op(int xc_handle, unsigned cmd, void *arg)
+{
+    int rc;
+    size_t arg_size;
+    DECLARE_HYPERCALL;
+
+    switch (cmd) {
+    case HVMOP_set_param:
+    case HVMOP_get_param:
+        arg_size = sizeof(xen_hvm_param_t);
+        break;
+    case HVMOP_set_pci_intx_level:
+        arg_size = sizeof(xen_hvm_set_pci_intx_level_t);
+        break;
+    case HVMOP_set_isa_irq_level:
+        arg_size = sizeof(xen_hvm_set_isa_irq_level_t);
+        break;
+    case HVMOP_set_pci_link_route:
+        arg_size = sizeof(xen_hvm_set_pci_link_route_t);
+        break;
+    case HVMOP_modified_memory:
+        arg_size = sizeof(xen_hvm_modified_memory_t);
+        break;
+    case HVMOP_set_mem_type:
+        arg_size = sizeof(xen_hvm_set_mem_type_t);
+        break;
+    case HVMOP_track_dirty_vram:
+        arg_size = sizeof(xen_hvm_track_dirty_vram_t);
+        break;
+    default:
+        errno = EINVAL;
+        return -1;
+    }
+
+    hypercall.op     = __HYPERVISOR_hvm_op;
+    hypercall.arg[0] = cmd;
+    hypercall.arg[1] = (unsigned long)arg;
+    if ( lock_pages(arg, arg_size) != 0 )
+	    return -1;
+    rc = do_xen_hypercall(xc_handle, &hypercall);
+    unlock_pages(&arg, arg_size);
+    return rc;
+}
+
 static void *minios_privcmd_map_foreign_bulk(xc_interface *xch, xc_osdep_handle h,
                                              uint32_t dom, int prot,
                                              const xen_pfn_t *arr, int *err, unsigned int num)
--- a/tools/libxc/xc_private.c
+++ b/tools/libxc/xc_private.c
@@ -246,6 +246,16 @@
     return xch->ops->u.privcmd.hypercall(xch, xch->ops_handle, hypercall);
 }
 
+int do_xen_arch_ioctl(xc_interface *xch, int req, long int arg)
+{
+    if (xch->ops->u.privcmd.arch_ioctl)
+        return xch->ops->u.privcmd.arch_ioctl(xch, xch->ops_handle, req, arg);
+
+    /* signal to caller that this interface is not available */
+    errno = EINVAL;
+    return -1;
+}
+
 xc_evtchn *xc_evtchn_open(xentoollog_logger *logger,
                              unsigned open_flags)
 {
@@ -505,26 +515,176 @@
     return flush_mmu_updates(xch, mmu);
 }
 
-int do_memory_op(xc_interface *xch, int cmd, void *arg, size_t len)
+int do_hvm_op(xc_interface *xch, unsigned cmd, void *arg)
 {
+    privcmd_hvmop_t pht;
+    size_t arg_size;
+    int rc;
+
+    errno = 0;
+
+    switch (cmd) {
+    case HVMOP_set_param:
+    case HVMOP_get_param:
+        arg_size = sizeof(pht.u.param);
+        break;
+    case HVMOP_set_pci_intx_level:
+        arg_size = sizeof(pht.u.set_pci_intx_level);
+        break;
+    case HVMOP_set_isa_irq_level:
+        arg_size = sizeof(pht.u.set_isa_irq_level);
+        break;
+    case HVMOP_set_pci_link_route:
+        arg_size = sizeof(pht.u.set_pci_link_route);
+        break;
+    case HVMOP_modified_memory:
+        arg_size = sizeof(pht.u.modified_memory);
+        break;
+    case HVMOP_set_mem_type:
+        arg_size = sizeof(pht.u.set_mem_type);
+        break;
+    case HVMOP_track_dirty_vram:
+        arg_size = sizeof(pht.u.track_dirty_vram);
+        break;
+    default:
+        ERROR("do_hvm_op: WARNING: unknown op=%d", cmd);
+        return -1;
+    }
+
+    if (errno == 0) {
+        pht.cmd = cmd;
+        memcpy(&pht.u, arg, arg_size);
+        rc = do_xen_arch_ioctl(xch, IOCTL_PRIVCMD_HVMOP, (long int)&pht);
+        if (rc >= 0) {
+            memcpy(arg, &pht.u, arg_size);
+            return rc;
+        }
+    }
+
+    if (errno == EINVAL) {
+        DECLARE_HYPERCALL;
+        DECLARE_HYPERCALL_BOUNCE(arg, arg_size, XC_HYPERCALL_BUFFER_BOUNCE_BOTH);
+
+        if ( xc_hypercall_bounce_pre(xch, arg) )
+        {
+            PERROR("Could not bounce memory for HVMOP hypercall");
+            return -1;
+        }
+
+        hypercall.op     = __HYPERVISOR_hvm_op;
+        hypercall.arg[0] = cmd;
+        hypercall.arg[1] = HYPERCALL_BUFFER_AS_ARG(arg);
+
+        rc = do_xen_hypercall(xch, &hypercall);
+        xc_hypercall_bounce_post(xch, arg);
+    }
+
+    return rc;
+}
+
+int do_platform(xc_interface *xch, struct xen_platform_op *pm)
+{
+    int ret = -1;
     DECLARE_HYPERCALL;
-    DECLARE_HYPERCALL_BOUNCE(arg, len, XC_HYPERCALL_BUFFER_BOUNCE_BOTH);
-    long ret = -EINVAL;
+    DECLARE_HYPERCALL_BOUNCE(pm, sizeof(*pm), XC_HYPERCALL_BUFFER_BOUNCE_BOTH);
 
-    if ( xc_hypercall_bounce_pre(xch, arg) )
+    pm->interface_version = XENPF_INTERFACE_VERSION;
+    if ( xc_hypercall_bounce_pre(xch, pm) )
     {
-        PERROR("Could not bounce memory for XENMEM hypercall");
-        goto out1;
+        PERROR("Could not bounce memory for platform op hypercall");
+        return -1;
     }
 
-    hypercall.op     = __HYPERVISOR_memory_op;
-    hypercall.arg[0] = (unsigned long) cmd;
-    hypercall.arg[1] = HYPERCALL_BUFFER_AS_ARG(arg);
+    hypercall.op     = __HYPERVISOR_platform_op;
+    hypercall.arg[0] = HYPERCALL_BUFFER_AS_ARG(pm);
+    if ( (ret = do_xen_hypercall(xch, &hypercall)) < 0 )
+    {
+        if ( errno == EACCES )
+            DPRINTF("platform operation failed -- need to"
+                    " rebuild the user-space tool set?\n");
+    }
 
-    ret = do_xen_hypercall(xch, &hypercall);
+    xc_hypercall_bounce_post(xch, pm);
+    return ret;
+}
+
+int do_memory_op(xc_interface *xch, int cmd, void *arg, size_t len)
+{
+    privcmd_memop_t pmt;
+    long ret = -EINVAL;
+    size_t arg_size;
+
+    errno = 0;
+
+    switch (cmd) {
+    case XENMEM_increase_reservation:
+    case XENMEM_decrease_reservation:
+    case XENMEM_populate_physmap:
+        arg_size = sizeof(pmt.u.reservation);
+        break;
+    case XENMEM_exchange:
+        arg_size = sizeof(pmt.u.exchange);
+        break;
+    case XENMEM_maximum_ram_page:
+        arg_size = 0;
+        break;
+    case XENMEM_current_reservation:
+    case XENMEM_maximum_reservation:
+    case XENMEM_maximum_gpfn:
+        arg_size = sizeof(pmt.u.domid);
+        break;
+    case XENMEM_machphys_mfn_list:
+        arg_size = sizeof(pmt.u.machphys_mfn_list);
+        break;
+    case XENMEM_machphys_mapping:
+        arg_size = sizeof(pmt.u.machphys_mapping);
+        break;
+    case XENMEM_add_to_physmap:
+        arg_size = sizeof(pmt.u.add_to_physmap);
+        break;
+    case XENMEM_memory_map:
+    case XENMEM_machine_memory_map:
+        arg_size = sizeof(pmt.u.memory_map);
+        break;
+    case XENMEM_set_memory_map:
+        arg_size = sizeof(pmt.u.foreign_memory_map);
+        break;
+    default:
+        ERROR("do_memory_op: WARNING: op=%d not converted to safe ioctls", cmd);
+        errno = EINVAL;
+        arg_size = len;
+        break;
+    }
+
+    if (errno == 0) {
+        pmt.cmd = cmd;
+        memcpy(&pmt.u, arg, arg_size);
+
+        ret = do_xen_arch_ioctl(xch, IOCTL_PRIVCMD_MEMOP, (long int)&pmt);
+        if (ret >= 0) {
+            memcpy(arg, &pmt.u, arg_size);
+            return ret;
+        }
+    }
+
+    if (errno == EINVAL) {
+        DECLARE_HYPERCALL;
+        DECLARE_HYPERCALL_BOUNCE(arg, len, XC_HYPERCALL_BUFFER_BOUNCE_BOTH);
+
+        if ( xc_hypercall_bounce_pre(xch, arg) )
+        {
+            PERROR("Could not bounce memory for XENMEM hypercall");
+            return -1;
+        }
+
+        hypercall.op     = __HYPERVISOR_memory_op;
+        hypercall.arg[0] = (unsigned long) cmd;
+        hypercall.arg[1] = HYPERCALL_BUFFER_AS_ARG(arg);
+
+        ret = do_xen_hypercall(xch, &hypercall);
+        xc_hypercall_bounce_post(xch, arg);
+    }
 
-    xc_hypercall_bounce_post(xch, arg);
- out1:
     return ret;
 }
 
@@ -566,9 +726,10 @@
 
     set_xen_guest_handle(xmml.extent_start, extent_start);
     rc = do_memory_op(xch, XENMEM_machphys_mfn_list, &xmml, sizeof(xmml));
-    if (rc || xmml.nr_extents != max_extents)
+    if (rc || xmml.nr_extents != max_extents) {
+        ERROR("xc_machphys_mfn_list: %ld != %ld", xmml.nr_extents, max_extents);
         rc = -1;
-    else
+    }else
         rc = 0;
 
     xc_hypercall_bounce_post(xch, extent_start);
--- a/tools/libxc/xc_private.h
+++ b/tools/libxc/xc_private.h
@@ -32,6 +32,7 @@
 #include "xenctrl.h"
 #include "xenctrlosdep.h"
 
+#include <xen/hvm/hvm_op.h>
 #include <xen/sys/privcmd.h>
 
 /* valgrind cannot see when a hypercall has filled in some values.  For this
@@ -189,6 +190,9 @@
  */
 
 int do_xen_hypercall(xc_interface *xch, privcmd_hypercall_t *hypercall);
+int do_xen_arch_ioctl(xc_interface *xch, int req, long int arg);
+int do_hvm_op(xc_interface *xch, unsigned cmd, void *arg);
+int do_platform(xc_interface *xch, struct xen_platform_op *pm);
 
 static inline int do_xen_version(xc_interface *xch, int cmd, xc_hypercall_buffer_t *dest)
 {
@@ -244,14 +248,21 @@
         goto out1;
     }
 
-    hypercall.op     = __HYPERVISOR_domctl;
-    hypercall.arg[0] = HYPERCALL_BUFFER_AS_ARG(domctl);
-
-    if ( (ret = do_xen_hypercall(xch, &hypercall)) < 0 )
-    {
-        if ( errno == EACCES )
-            DPRINTF("domctl operation failed -- need to"
-                    " rebuild the user-space tool set?\n");
+    /* Try the new interface first, then fall back to the old one. */
+    ret = do_xen_arch_ioctl(xch, IOCTL_PRIVCMD_DOMCTL,
+                     HYPERCALL_BUFFER_AS_ARG(domctl));
+    if (ret < 0 && errno == EINVAL) {
+        /* The new interface failed, and it's likely to be because
+           it's not available.  Try the old one. */
+        hypercall.op     = __HYPERVISOR_domctl;
+        hypercall.arg[0] = HYPERCALL_BUFFER_AS_ARG(domctl);
+
+        if ( (ret = do_xen_hypercall(xch, &hypercall)) < 0 )
+        {
+            if ( errno == EACCES )
+                DPRINTF("domctl operation failed -- need to"
+                        " rebuild the user-space tool set?\n");
+        }
     }
 
     xc_hypercall_bounce_post(xch, domctl);
--- a/tools/libxc/xenctrlosdep.h
+++ b/tools/libxc/xenctrlosdep.h
@@ -89,6 +89,8 @@
             void *(*map_foreign_ranges)(xc_interface *xch, xc_osdep_handle h, uint32_t dom, size_t size, int prot,
                                         size_t chunksize, privcmd_mmap_entry_t entries[],
                                         int nentries);
+            int (*arch_ioctl)(xc_interface *xch, xc_osdep_handle h, int cmd, long int arg);
+            int (*restrict_to)(xc_interface *xch, xc_osdep_handle h, domid_t domid);
         } privcmd;
         struct {
             int (*fd)(xc_evtchn *xce, xc_osdep_handle h);
@@ -104,6 +106,7 @@
 
             evtchn_port_or_error_t (*pending)(xc_evtchn *xce, xc_osdep_handle h);
             int (*unmask)(xc_evtchn *xce, xc_osdep_handle h, evtchn_port_t port);
+            int (*restrict_to)(xc_interface *xch, xc_osdep_handle h, domid_t domid);
         } evtchn;
         struct {
 #define XC_GRANT_MAP_SINGLE_DOMAIN 0x1
--- a/tools/libxc/xenguest.h
+++ b/tools/libxc/xenguest.h
@@ -215,4 +215,8 @@
                       unsigned long max_mfn,
                       int prot,
                       unsigned long *mfn0);
+
+/* Accessors for struct xc_dom_image */
+const char *xc_dom_get_native_protocol(struct xc_dom_image *dom);
+
 #endif /* XENGUEST_H */
--- a/tools/ocaml/libs/xc/xenctrl.ml
+++ b/tools/ocaml/libs/xc/xenctrl.ml
@@ -28,6 +28,18 @@
 	cpumap: int32;
 }
 
+type runstateinfo = {
+  state : int32;
+  missed_changes: int32;
+  state_entry_time : int64;
+  time0 : int64;
+  time1 : int64;
+  time2 : int64;
+  time3 : int64;
+  time4 : int64;
+  time5 : int64;
+}
+
 type domaininfo =
 {
 	domid             : domid;
@@ -164,6 +176,8 @@
 
 external domain_get_vcpuinfo: handle -> int -> int -> vcpuinfo
        = "stub_xc_vcpu_getinfo"
+external domain_get_runstate_info : handle -> int -> runstateinfo
+  = "stub_xc_get_runstate_info"
 
 external domain_ioport_permission: handle -> domid -> int -> int -> bool -> unit
        = "stub_xc_domain_ioport_permission"
@@ -237,6 +251,23 @@
 external domain_test_assign_device: handle -> domid -> (int * int * int * int) -> bool
        = "stub_xc_domain_test_assign_device"
 
+external domain_suppress_spurious_page_faults: handle -> domid -> unit
+       = "stub_xc_domain_suppress_spurious_page_faults"
+
+external domain_set_timer_mode: handle -> domid -> int -> unit = "stub_xc_domain_set_timer_mode"
+external domain_set_hpet: handle -> domid -> int -> unit = "stub_xc_domain_set_hpet"
+external domain_set_vpt_align: handle -> domid -> int -> unit = "stub_xc_domain_set_vpt_align"
+
+external domain_send_s3resume: handle -> domid -> unit = "stub_xc_domain_send_s3resume"
+external domain_get_acpi_s_state: handle -> domid -> int = "stub_xc_domain_get_acpi_s_state"
+
+external domain_trigger_power: handle -> domid -> unit = "stub_xc_domain_trigger_power"
+external domain_trigger_sleep: handle -> domid -> unit = "stub_xc_domain_trigger_sleep"
+
+(** check if some hvm domain got pv driver or not *)
+external hvm_check_pvdriver: handle -> domid -> bool
+       = "stub_xc_hvm_check_pvdriver"
+
 external version: handle -> version = "stub_xc_version_version"
 external version_compile_info: handle -> compile_info
        = "stub_xc_version_compile_info"
@@ -247,6 +278,9 @@
 external watchdog : handle -> int -> int32 -> int
   = "stub_xc_watchdog"
 
+external get_boot_cpufeatures: handle ->
+        (int32 * int32 * int32 * int32 * int32 * int32 * int32 * int32) = "stub_xc_get_boot_cpufeatures"
+
 (* core dump structure *)
 type core_magic = Magic_hvm | Magic_pv
 
--- a/tools/ocaml/libs/xc/xenctrl.mli
+++ b/tools/ocaml/libs/xc/xenctrl.mli
@@ -22,6 +22,17 @@
   cputime : int64;
   cpumap : int32;
 }
+type runstateinfo = {
+  state : int32;
+  missed_changes: int32;
+  state_entry_time : int64;
+  time0 : int64;
+  time1 : int64;
+  time2 : int64;
+  time3 : int64;
+  time4 : int64;
+  time5 : int64;
+}
 type domaininfo = {
   domid : domid;
   dying : bool;
@@ -93,6 +104,8 @@
   = "stub_xc_domain_getinfo"
 external domain_get_vcpuinfo : handle -> int -> int -> vcpuinfo
   = "stub_xc_vcpu_getinfo"
+external domain_get_runstate_info : handle -> int -> runstateinfo
+  = "stub_xc_get_runstate_info"
 external domain_ioport_permission: handle -> domid -> int -> int -> bool -> unit
        = "stub_xc_domain_ioport_permission"
 external domain_iomem_permission: handle -> domid -> nativeint -> nativeint -> bool -> unit
@@ -142,6 +155,21 @@
 external domain_test_assign_device: handle -> domid -> (int * int * int * int) -> bool
        = "stub_xc_domain_test_assign_device"
 
+external domain_set_timer_mode: handle -> domid -> int -> unit = "stub_xc_domain_set_timer_mode"
+external domain_set_hpet: handle -> domid -> int -> unit = "stub_xc_domain_set_hpet"
+external domain_set_vpt_align: handle -> domid -> int -> unit = "stub_xc_domain_set_vpt_align"
+
+external domain_send_s3resume: handle -> domid -> unit
+  = "stub_xc_domain_send_s3resume"
+external domain_get_acpi_s_state: handle -> domid -> int = "stub_xc_domain_get_acpi_s_state"
+
+external domain_trigger_power: handle -> domid -> unit
+  = "stub_xc_domain_trigger_power"
+external domain_trigger_sleep: handle -> domid -> unit
+  = "stub_xc_domain_trigger_sleep"
+
+external hvm_check_pvdriver : handle -> domid -> bool
+  = "stub_xc_hvm_check_pvdriver"
 external version : handle -> version = "stub_xc_version_version"
 external version_compile_info : handle -> compile_info
   = "stub_xc_version_compile_info"
@@ -170,6 +198,9 @@
 external domain_get_machine_address_size: handle -> domid -> int
        = "stub_xc_domain_get_machine_address_size"
 
+external domain_suppress_spurious_page_faults: handle -> domid -> unit
+       = "stub_xc_domain_suppress_spurious_page_faults"
+
 external domain_cpuid_set: handle -> domid -> (int64 * (int64 option))
                         -> string option array
                         -> string option array
@@ -179,3 +210,5 @@
 external cpuid_check: handle -> (int64 * (int64 option)) -> string option array -> (bool * string option array)
        = "stub_xc_cpuid_check"
 
+external get_boot_cpufeatures: handle ->
+        (int32 * int32 * int32 * int32 * int32 * int32 * int32 * int32) = "stub_xc_get_boot_cpufeatures"
--- a/tools/ocaml/libs/xc/xenctrl_stubs.c
+++ b/tools/ocaml/libs/xc/xenctrl_stubs.c
@@ -389,6 +389,37 @@
 	CAMLreturn(result);
 }
 
+CAMLprim value stub_xc_get_runstate_info(value xch, value domid)
+{
+        CAMLparam2(xch, domid);
+        CAMLlocal1(result);
+        xc_runstate_info_t info;
+        int retval;
+
+        retval = xc_get_runstate_info(_H(xch), _D(domid), &info);
+        if (retval < 0)
+                failwith_xc(_H(xch));
+
+        /* Store
+           0 : state (int32)
+           1 : missed_changes (int32)
+           2 : state_entry_time (int64)
+           3-8 : times (int64s)
+        */
+        result = caml_alloc_tuple(9);
+        Store_field(result, 0, caml_copy_int32(info.state));
+        Store_field(result, 1, caml_copy_int32(info.missed_changes));
+        Store_field(result, 2, caml_copy_int64(info.state_entry_time));
+        Store_field(result, 3, caml_copy_int64(info.time[0]));
+        Store_field(result, 4, caml_copy_int64(info.time[1]));
+        Store_field(result, 5, caml_copy_int64(info.time[2]));
+        Store_field(result, 6, caml_copy_int64(info.time[3]));
+        Store_field(result, 7, caml_copy_int64(info.time[4]));
+        Store_field(result, 8, caml_copy_int64(info.time[5]));
+
+        CAMLreturn(result);
+}
+
 CAMLprim value stub_xc_vcpu_context_get(value xch, value domid,
                                         value cpu)
 {
@@ -701,6 +732,17 @@
 	CAMLreturn(Val_int(retval));
 }
 
+CAMLprim value stub_xc_domain_suppress_spurious_page_faults(value xch,
+							    value domid)
+{
+	CAMLparam2(xch, domid);
+
+	int retval = xc_domain_suppress_spurious_page_faults(_H(xch), _D(domid));
+	if (retval)
+		failwith_xc(_H(xch));
+	CAMLreturn(Val_unit);
+}
+
 CAMLprim value stub_xc_domain_cpuid_set(value xch, value domid,
                                         value input,
                                         value config)
@@ -1083,6 +1125,17 @@
 	return bdf;
 }
 
+CAMLprim value stub_xc_hvm_check_pvdriver(value xch, value domid)
+{
+	CAMLparam2(xch, domid);
+	int ret;
+
+	ret = xc_hvm_check_pvdriver(_H(xch), _D(domid));
+	if (ret < 0)
+		failwith_xc(_H(xch));
+	CAMLreturn(Val_bool(ret));
+}
+
 CAMLprim value stub_xc_domain_test_assign_device(value xch, value domid, value desc)
 {
 	CAMLparam3(xch, domid, desc);
@@ -1154,6 +1207,97 @@
 	CAMLreturn(Val_int(ret));
 }
 
+CAMLprim value stub_xc_domain_get_acpi_s_state(value xch, value domid)
+{
+        CAMLparam2(xch, domid);
+        int ret;
+
+        ret = xc_domain_get_acpi_s_state(_H(xch), _D(domid));
+        if (ret < 0)
+                failwith_xc(_H(xch));
+
+        CAMLreturn(Val_int(ret));
+}
+
+CAMLprim value stub_xc_domain_send_s3resume(value xch, value domid)
+{
+        CAMLparam2(xch, domid);
+        xc_domain_send_s3resume(_H(xch), _D(domid));
+        CAMLreturn(Val_unit);
+}
+
+
+CAMLprim value stub_xc_domain_set_timer_mode(value xch, value id, value mode)
+{
+        CAMLparam3(xch, id, mode);
+        int ret;
+
+        ret = xc_domain_set_timer_mode(_H(xch), _D(id), Int_val(mode));
+        if (ret < 0)
+                failwith_xc(_H(xch));
+        CAMLreturn(Val_unit);
+}
+
+CAMLprim value stub_xc_domain_set_hpet(value xch, value id, value mode)
+{
+        CAMLparam3(xch, id, mode);
+        int ret;
+
+        ret = xc_domain_set_hpet(_H(xch), _D(id), Int_val(mode));
+        if (ret < 0)
+                failwith_xc(_H(xch));
+        CAMLreturn(Val_unit);
+}
+
+CAMLprim value stub_xc_domain_set_vpt_align(value xch, value id, value mode)
+{
+        CAMLparam3(xch, id, mode);
+        int ret;
+
+        ret = xc_domain_set_vpt_align(_H(xch), _D(id), Int_val(mode));
+        if (ret < 0)
+                failwith_xc(_H(xch));
+        CAMLreturn(Val_unit);
+}
+
+CAMLprim value stub_xc_domain_trigger_power(value xch, value domid)
+{
+	CAMLparam2(xch, domid);
+	xc_domain_trigger_power(_H(xch), _D(domid));
+	CAMLreturn(Val_unit);
+}
+
+CAMLprim value stub_xc_domain_trigger_sleep(value xch, value domid)
+{
+	CAMLparam2(xch, domid);
+	xc_domain_trigger_sleep(_H(xch), _D(domid));
+	CAMLreturn(Val_unit);
+}
+
+CAMLprim value stub_xc_get_boot_cpufeatures(value xch)
+{
+	CAMLparam1(xch);
+	CAMLlocal1(v);
+	uint32_t a, b, c, d, e, f, g, h;
+	int ret;
+
+	ret = xc_get_boot_cpufeatures(_H(xch), &a, &b, &c, &d, &e, &f, &g, &h);
+	if (ret < 0)
+		failwith_xc(_H(xch));
+
+	v = caml_alloc_tuple(8);
+	Store_field(v, 0, caml_copy_int32(a));
+	Store_field(v, 1, caml_copy_int32(b));
+	Store_field(v, 2, caml_copy_int32(c));
+	Store_field(v, 3, caml_copy_int32(d));
+	Store_field(v, 4, caml_copy_int32(e));
+	Store_field(v, 5, caml_copy_int32(f));
+	Store_field(v, 6, caml_copy_int32(g));
+	Store_field(v, 7, caml_copy_int32(h));
+
+	CAMLreturn(v);
+}
+
 /*
  * Local variables:
  *  indent-tabs-mode: t
--- a/xen/arch/x86/cpu/amd.c
+++ b/xen/arch/x86/cpu/amd.c
@@ -89,12 +89,16 @@
  */
 static void __devinit set_cpuidmask(const struct cpuinfo_x86 *c)
 {
+	unsigned int eax, ebx;
 	static unsigned int feat_ecx, feat_edx;
 	static unsigned int extfeat_ecx, extfeat_edx;
 	static enum { not_parsed, no_mask, set_mask } status;
 
+	cpuid(0x1, &eax, &ebx, &c->boot_base_ecx, &c->boot_base_edx);
+	cpuid(0x80000001, &eax, &ebx, &c->boot_ext_ecx, &c->boot_ext_edx);
+
 	if (status == no_mask)
-		return;
+		goto out;
 
 	if (status == set_mask)
 		goto setmask;
@@ -109,7 +113,7 @@
 		extfeat_ecx = opt_cpuid_mask_ext_ecx;
 		extfeat_edx = opt_cpuid_mask_ext_edx;
 	} else if (*opt_famrev == '\0') {
-		return;
+		goto out;
 	} else if (!strcmp(opt_famrev, "fam_0f_rev_c")) {
 		feat_ecx = AMD_FEATURES_K8_REV_C_ECX;
 		feat_edx = AMD_FEATURES_K8_REV_C_EDX;
@@ -153,7 +157,7 @@
 	} else {
 		printk("Invalid processor string: %s\n", opt_famrev);
 		printk("CPUID will not be masked\n");
-		return;
+		goto out;
 	}
 
         /* Setting bits in the CPUID mask MSR that are not set in the
@@ -180,6 +184,10 @@
 		wrmsr_amd(MSR_K8_FEATURE_MASK, feat_edx, feat_ecx);
 		wrmsr_amd(MSR_K8_EXT_FEATURE_MASK, extfeat_edx, extfeat_ecx);
 	}
+
+out:
+	cpuid(0x1, &eax, &ebx, &c->masked_base_ecx, &c->masked_base_edx);
+	cpuid(0x80000001, &eax, &ebx, &c->masked_ext_ecx, &c->masked_ext_edx);
 }
 
 /*
--- a/xen/arch/x86/cpu/intel.c
+++ b/xen/arch/x86/cpu/intel.c
@@ -57,13 +57,16 @@
  */
 static void __devinit set_cpuidmask(const struct cpuinfo_x86 *c)
 {
-	u32 eax, edx;
+	u32 eax, ebx, edx;
 	const char *extra = "";
 
+	cpuid(0x1, &eax, &ebx, &c->boot_base_ecx, &c->boot_base_edx);
+	cpuid(0x80000001, &eax, &ebx, &c->boot_ext_ecx, &c->boot_ext_edx);
+
 	if (!~(opt_cpuid_mask_ecx & opt_cpuid_mask_edx &
 	       opt_cpuid_mask_ext_ecx & opt_cpuid_mask_ext_edx &
                opt_cpuid_mask_xsave_eax))
-		return;
+        goto out;
 
 	/* Only family 6 supports this feature  */
 	switch ((c->x86 == 6) * c->x86_model) {
@@ -119,6 +122,9 @@
 
 	printk(XENLOG_ERR "Cannot set CPU %sfeature mask on CPU#%d\n",
 	       extra, smp_processor_id());
+out:
+    cpuid(0x1, &eax, &ebx, &c->masked_base_ecx, &c->masked_base_edx);
+    cpuid(0x80000001, &eax, &ebx, &c->masked_ext_ecx, &c->masked_ext_edx);
 }
 
 void __devinit early_intel_workaround(struct cpuinfo_x86 *c)
--- a/xen/arch/x86/hvm/svm/svm.c
+++ b/xen/arch/x86/hvm/svm/svm.c
@@ -1213,15 +1213,36 @@
 {
     unsigned int input = *eax;
     struct vcpu *v = current;
+    unsigned int cores_per_socket = current->domain->cores_per_socket;
 
     hvm_cpuid(input, eax, ebx, ecx, edx);
 
-    switch (input) {
+    switch ( input ) 
+    {
+    case 0x00000001:
+        if ( cores_per_socket > 1 ) 
+        {
+            *ebx &= 0xFF00FFFF;
+            *ebx |= (2 * cores_per_socket & 0xFF) << 16;
+            *edx |= 0x1 << 28;
+        }
+        break;
+
     case 0x80000001:
         /* Fix up VLAPIC details. */
         if ( vlapic_hw_disabled(vcpu_vlapic(v)) )
             __clear_bit(X86_FEATURE_APIC & 31, edx);
+        if (cores_per_socket > 1)
+            *ecx |= cpufeat_mask(X86_FEATURE_CMP_LEGACY);
         break;
+
+    case 0x80000008:
+        /* Make sure Number of CPU core is 1 when HTT=0 */
+        *ecx &= 0xFFFF0F00;
+        if ( cores_per_socket > 1 )
+            *ecx |= (2 * cores_per_socket - 1) & 0xFF;
+        break;
+
     case 0x8000001c: 
     {
         /* LWP capability CPUID */
--- a/xen/arch/x86/hvm/vmx/vmx.c
+++ b/xen/arch/x86/hvm/vmx/vmx.c
@@ -1580,20 +1580,40 @@
     unsigned int input = *eax;
     struct segment_register cs;
     struct vcpu *v = current;
+    unsigned int cores_per_socket = current->domain->cores_per_socket;
 
     hvm_cpuid(input, eax, ebx, ecx, edx);
 
     switch ( input )
     {
-        case 0x80000001:
-            /* SYSCALL is visible iff running in long mode. */
-            hvm_get_segment_register(v, x86_seg_cs, &cs);
-            if ( cs.attr.fields.l )
-                *edx |= cpufeat_mask(X86_FEATURE_SYSCALL);
-            else
-                *edx &= ~(cpufeat_mask(X86_FEATURE_SYSCALL));
+    case 0x00000001:
+        if ( cores_per_socket > 1 )
+        {
+            /* to fake out #vcpus per socket first force on HT/MC */
+            *edx |= cpufeat_mask(X86_FEATURE_HT);
+            /* and then inform guest of #cores per package */
+            *ebx &= 0xFF00FFFF;
+            *ebx |= (((cores_per_socket * 2) & 0xFF) << 16);
+        }
+        break;
 
-            break;
+    case 0x00000004:         
+        if (cores_per_socket > 1) {
+            /* fake out cores per socket */
+            *eax &= 0x3FFF; /* one thread, one core */
+            *eax |= (((cores_per_socket * 2) - 1) << 26);
+        }
+        break;
+
+    case 0x80000001:
+        /* SYSCALL is visible iff running in long mode. */
+        hvm_get_segment_register(v, x86_seg_cs, &cs);
+        if ( cs.attr.fields.l )
+            *edx |= cpufeat_mask(X86_FEATURE_SYSCALL);
+        else
+            *edx &= ~(cpufeat_mask(X86_FEATURE_SYSCALL));
+        
+        break;
     }
 
     HVMTRACE_5D (CPUID, input, *eax, *ebx, *ecx, *edx);
--- a/xen/arch/x86/platform_hypercall.c
+++ b/xen/arch/x86/platform_hypercall.c
@@ -518,6 +518,35 @@
                       op->u.mem_add.epfn,
                       op->u.mem_add.pxm);
         break;
+
+    case XENPF_get_cpu_features:
+    {
+        uint32_t cpu;
+
+        op->u.cpu_features.base_ecx = 0xffffffff;
+        op->u.cpu_features.base_edx = 0xffffffff;
+        op->u.cpu_features.ext_ecx = 0xffffffff;
+        op->u.cpu_features.ext_edx = 0xffffffff;
+        op->u.cpu_features.masked_base_ecx = 0xffffffff;
+        op->u.cpu_features.masked_base_edx = 0xffffffff;
+        op->u.cpu_features.masked_ext_ecx = 0xffffffff;
+        op->u.cpu_features.masked_ext_edx = 0xffffffff;
+        for_each_online_cpu( cpu )
+        {
+            op->u.cpu_features.base_ecx &= cpu_data[cpu].boot_base_ecx;
+            op->u.cpu_features.base_edx &= cpu_data[cpu].boot_base_edx;
+            op->u.cpu_features.ext_ecx  &= cpu_data[cpu].boot_ext_ecx;
+            op->u.cpu_features.ext_edx  &= cpu_data[cpu].boot_ext_edx;
+            op->u.cpu_features.masked_base_ecx &= cpu_data[cpu].masked_base_ecx;
+            op->u.cpu_features.masked_base_edx &= cpu_data[cpu].masked_base_edx;
+            op->u.cpu_features.masked_ext_ecx  &= cpu_data[cpu].masked_ext_ecx;
+            op->u.cpu_features.masked_ext_edx  &= cpu_data[cpu].masked_ext_edx;
+        }
+
+        ret = copy_to_guest(u_xenpf_op, op, 1) ? -EFAULT : 0;
+    }
+    break;
+
     default:
         ret = -ENOSYS;
         break;
--- a/xen/common/domain.c
+++ b/xen/common/domain.c
@@ -247,6 +247,7 @@
     spin_lock_init_prof(d, domain_lock);
     spin_lock_init_prof(d, page_alloc_lock);
     spin_lock_init(&d->hypercall_deadlock_mutex);
+    spin_lock_init(&d->runstate_lock);
     INIT_PAGE_LIST_HEAD(&d->page_list);
     INIT_PAGE_LIST_HEAD(&d->xenpage_list);
 
--- a/xen/common/domctl.c
+++ b/xen/common/domctl.c
@@ -966,6 +966,40 @@
     }
     break;
 
+    case XEN_DOMCTL_get_runstate_info:
+    {
+        struct domain *d;
+
+        ret = -ESRCH;
+        d = rcu_lock_domain_by_id(op->domain);
+        if ( d != NULL )
+        {
+            domain_runstate_get(d, &op->u.domain_runstate);
+            ret = 0;
+
+            rcu_unlock_domain(d);
+
+            if ( copy_to_guest(u_domctl, op, 1) )
+                ret = -EFAULT;
+        }
+        break;
+    }
+
+    case XEN_DOMCTL_setcorespersocket:
+    {
+        struct domain *d;
+
+        ret = -ESRCH;
+        d = rcu_lock_domain_by_id(op->domain);
+        if ( d != NULL )
+        {
+            d->cores_per_socket = op->u.setcorespersocket.cores_per_socket;
+            rcu_unlock_domain(d);
+            ret = 0;
+        }
+    }
+    break;
+
     default:
         ret = arch_do_domctl(op, u_domctl);
         break;
--- a/xen/common/schedule.c
+++ b/xen/common/schedule.c
@@ -132,9 +132,30 @@
     }
 }
 
+/* Used to quickly map the vcpu runstate mask to a domain runstate */
+static int mask_to_state[] = {
+    /* 000: Nothing in any runstate.  Should never happen. */
+    -1,
+    /* 001: All running */
+    DOMAIN_RUNSTATE_full_run,
+    /* 010: All runnable */
+    DOMAIN_RUNSTATE_full_contention,
+    /* 011: Some running, some runnable */
+    DOMAIN_RUNSTATE_concurrency_hazard,
+    /* 100: All blocked / offline */
+    DOMAIN_RUNSTATE_blocked,
+    /* 101: Some running, some blocked / offline */
+    DOMAIN_RUNSTATE_partial_run,
+    /* 110: Some blocked / offline, some runnable */
+    DOMAIN_RUNSTATE_partial_contention,
+    /* 111: Some in every state.  Mixed running + runnable is most important. */
+    DOMAIN_RUNSTATE_concurrency_hazard
+};
+
 static inline void vcpu_runstate_change(
     struct vcpu *v, int new_state, s_time_t new_entry_time)
 {
+    struct domain *d = v->domain;
     s_time_t delta;
 
     ASSERT(v->runstate.state != new_state);
@@ -152,6 +173,45 @@
     }
 
     v->runstate.state = new_state;
+
+    /* Update domain runstate */
+    if ( spin_trylock(&d->runstate_lock) )
+    {
+        unsigned mask=0;
+        struct vcpu *ov;
+
+        BUG_ON(d->runstate.state > DOMAIN_RUNSTATE_partial_contention);
+
+        d->runstate.time[d->runstate.state] +=
+            (new_entry_time - d->runstate.state_entry_time);
+        d->runstate.state_entry_time = new_entry_time;
+
+        /* Determine new runstate.  First, see what states we have */
+        for_each_vcpu(d, ov) {
+            /* Don't count vcpus that have beent taken offline by the guest */ 
+            if ( !(ov->runstate.state == RUNSTATE_offline
+                   && test_bit(_VPF_down, &ov->pause_flags)) )
+               mask |= (1 << ov->runstate.state);
+        }
+
+        if ( mask == 0 )
+        {
+            printk("%s: d%d has no online vcpus!\n",
+                   __func__, d->domain_id);
+            mask = 1 << RUNSTATE_offline;
+        }
+
+        /* Offline & blocked are the same */
+        mask |= ((1 << RUNSTATE_offline) & mask) >> 1;
+
+        d->runstate.state = mask_to_state[mask&0x7];
+
+        spin_unlock(&d->runstate_lock);
+    } 
+    else 
+    {
+        atomic_inc(&d->runstate_missed_changes);
+    }
 }
 
 void vcpu_runstate_get(struct vcpu *v, struct vcpu_runstate_info *runstate)
@@ -170,6 +230,20 @@
         vcpu_schedule_unlock_irq(v);
 }
 
+void domain_runstate_get(struct domain *d, domain_runstate_info_t *runstate)
+{
+    unsigned long flags;
+    /* Have to disable interrupts because the other user of the lock runs 
+     * in interrupt context. */
+    spin_lock_irqsave(&d->runstate_lock, flags);
+
+    memcpy(runstate, &d->runstate, sizeof(*runstate));
+    runstate->time[d->runstate.state] += NOW() - runstate->state_entry_time;
+    runstate->missed_changes = atomic_read(&d->runstate_missed_changes);
+
+    spin_unlock_irqrestore(&d->runstate_lock, flags);
+}
+
 uint64_t get_cpu_idle_time(unsigned int cpu)
 {
     struct vcpu_runstate_info state;
--- a/xen/include/asm-x86/processor.h
+++ b/xen/include/asm-x86/processor.h
@@ -167,6 +167,10 @@
     __u8 x86_mask;
     int  cpuid_level;    /* Maximum supported CPUID level, -1=no CPUID */
     unsigned int x86_capability[NCAPINTS];
+    unsigned int boot_base_ecx, boot_base_edx;
+    unsigned int boot_ext_ecx, boot_ext_edx;
+    unsigned int masked_base_ecx, masked_base_edx;
+    unsigned int masked_ext_ecx, masked_ext_edx;
     char x86_vendor_id[16];
     char x86_model_id[64];
     int  x86_cache_size; /* in KB - valid for CPUS which support this call  */
--- a/xen/include/public/domctl.h
+++ b/xen/include/public/domctl.h
@@ -835,6 +835,53 @@
 typedef struct xen_domctl_set_access_required xen_domctl_set_access_required_t;
 DEFINE_XEN_GUEST_HANDLE(xen_domctl_set_access_required_t);
 
+/*
+ * Return information about the state and running time of a domain.
+ * The "domain runstate" is based on the runstates of all the vcpus of the
+ * domain (see below).
+ * @extra_arg == pointer to domain_runstate_info structure.
+ */
+struct xen_domctl_runstate_info {
+    /* VCPU's current state (RUNSTATE_*). */
+    uint32_t      state;
+    uint32_t missed_changes;
+    /* Number of times we missed an update due to contention */
+    /* When was current state entered (system time, ns)? */
+    uint64_t state_entry_time;
+    /*
+     * Time spent in each RUNSTATE_* (ns). The sum of these times is
+     * NOT guaranteed not to drift from system time.
+     */
+    uint64_t time[6];
+};
+typedef struct xen_domctl_runstate_info xen_domctl_runstate_info_t;
+DEFINE_XEN_GUEST_HANDLE(xen_domctl_runstate_info_t);
+
+/* All vcpus are running */
+#define DOMAIN_RUNSTATE_full_run           0
+
+/* All vcpus are runnable (i.e., waiting for cpu) */
+#define DOMAIN_RUNSTATE_full_contention    1
+
+/* Some vcpus are running, some are runnable */
+#define DOMAIN_RUNSTATE_concurrency_hazard 2
+
+/* All vcpus are blocked / offline */
+#define DOMAIN_RUNSTATE_blocked            3
+
+/* Some vpcus are running, some are blocked */
+#define DOMAIN_RUNSTATE_partial_run        4
+
+/* Some vcpus are runnable, some are blocked */
+#define DOMAIN_RUNSTATE_partial_contention 5
+
+struct xen_domctl_setcorespersocket {
+    uint32_t cores_per_socket;
+};
+
+typedef struct xen_domctl_setcorespersocket xen_domctl_setcorespersocket_t;
+DEFINE_XEN_GUEST_HANDLE(xen_domctl_setcorespersocket_t);
+
 struct xen_domctl {
     uint32_t cmd;
 #define XEN_DOMCTL_createdomain                   1
@@ -898,6 +945,8 @@
 #define XEN_DOMCTL_setvcpuextstate               62
 #define XEN_DOMCTL_getvcpuextstate               63
 #define XEN_DOMCTL_set_access_required           64
+#define XEN_DOMCTL_get_runstate_info             98
+#define XEN_DOMCTL_setcorespersocket             99
 #define XEN_DOMCTL_gdbsx_guestmemio            1000
 #define XEN_DOMCTL_gdbsx_pausevcpu             1001
 #define XEN_DOMCTL_gdbsx_unpausevcpu           1002
@@ -951,6 +1000,8 @@
         struct xen_domctl_vcpuextstate      vcpuextstate;
 #endif
         struct xen_domctl_set_access_required access_required;
+        struct xen_domctl_runstate_info     domain_runstate;
+        struct xen_domctl_setcorespersocket setcorespersocket;
         struct xen_domctl_gdbsx_memio       gdbsx_guest_memio;
         struct xen_domctl_gdbsx_pauseunp_vcpu gdbsx_pauseunp_vcpu;
         struct xen_domctl_gdbsx_domstatus   gdbsx_domstatus;
--- a/xen/include/public/platform.h
+++ b/xen/include/public/platform.h
@@ -475,6 +475,24 @@
     uint32_t flags;
 };
 
+
+/* Get the CPUID feature lists before and after any hardware masks 
+ * were applied.   Returns the ANDed aggregate of all online CPUs. */
+#define XENPF_get_cpu_features  511
+struct xenpf_cpu_features {
+    uint32_t base_ecx;          /* CPUID leaf 0x00000001:ECX */
+    uint32_t base_edx;          /* CPUID leaf 0x00000001:EDX */
+    uint32_t ext_ecx;           /* CPUID leaf 0x80000001:ECX */
+    uint32_t ext_edx;           /* CPUID leaf 0x80000001:EDX */
+    uint32_t masked_base_ecx;   /* CPUID leaf 0x00000001:ECX */
+    uint32_t masked_base_edx;   /* CPUID leaf 0x00000001:EDX */
+    uint32_t masked_ext_ecx;    /* CPUID leaf 0x80000001:ECX */
+    uint32_t masked_ext_edx;    /* CPUID leaf 0x80000001:EDX */
+};
+typedef struct xenpf_cpu_features xenpf_cpu_features_t;
+DEFINE_XEN_GUEST_HANDLE(xenpf_cpu_features_t);
+
+
 struct xen_platform_op {
     uint32_t cmd;
     uint32_t interface_version; /* XENPF_INTERFACE_VERSION */
@@ -495,6 +513,7 @@
         struct xenpf_cpu_ol            cpu_ol;
         struct xenpf_cpu_hotadd        cpu_add;
         struct xenpf_mem_hotadd        mem_add;
+        struct xenpf_cpu_features      cpu_features;
         uint8_t                        pad[128];
     } u;
 };
--- a/xen/include/xen/sched.h
+++ b/xen/include/xen/sched.h
@@ -194,6 +194,8 @@
     int xen_port;
 };
 
+typedef struct xen_domctl_runstate_info domain_runstate_info_t;
+
 struct domain
 {
     domid_t          domain_id;
@@ -329,6 +331,13 @@
     nodemask_t node_affinity;
     unsigned int last_alloc_node;
     spinlock_t node_affinity_lock;
+
+    /* Domain runstates */
+    spinlock_t runstate_lock;
+    atomic_t runstate_missed_changes;
+    domain_runstate_info_t runstate;
+
+    unsigned int cores_per_socket;
 };
 
 struct domain_setup_info
@@ -620,6 +629,8 @@
 int vcpu_set_affinity(struct vcpu *v, const cpumask_t *affinity);
 
 void vcpu_runstate_get(struct vcpu *v, struct vcpu_runstate_info *runstate);
+void domain_runstate_get(struct domain *d, domain_runstate_info_t *runstate);
+
 uint64_t get_cpu_idle_time(unsigned int cpu);
 
 /*
--- a/tools/firmware/hvmloader/acpi/build.c
+++ b/tools/firmware/hvmloader/acpi/build.c
@@ -285,6 +285,41 @@
     return nr_tables;
 }
 
+static void remove_block_from_dsdt(uint8_t *dsdt, uint8_t *sig, int block_size)
+{
+    struct acpi_header *dsdt_header;
+    int i;
+
+    dsdt_header = (struct acpi_header*)dsdt;
+
+    for ( i = 0; i < dsdt_header->length-block_size; i++ ) 
+    {
+        if ( memcmp(dsdt+i, sig, 4) == 0 ) 
+        {
+            memcpy(dsdt+i, dsdt+i+block_size, dsdt_header->length-i-block_size);
+            dsdt_header->length -= block_size;
+            break;
+       }
+    }
+}
+
+static void update_dsdt(uint8_t *dsdt)
+{
+    unsigned char sigS4[] = {'_', 'S', '4', '_'};
+    unsigned char sigS3[] = {'_', 'S', '3', '_'};
+
+    if ( get_s4_enabled() == 0 )
+        remove_block_from_dsdt(dsdt, sigS4, 14);
+
+    if ( get_s3_enabled() == 0 )
+        remove_block_from_dsdt(dsdt, sigS3, 14);
+
+    set_checksum(dsdt,
+                 offsetof(struct acpi_header, checksum),
+                 ((struct acpi_header*)dsdt)->length);
+}
+
+
 void acpi_build_tables(unsigned int physical)
 {
     struct acpi_info *acpi_info = (struct acpi_info *)ACPI_INFO_PHYSICAL_ADDRESS;
@@ -322,6 +357,7 @@
         memcpy(dsdt, &dsdt_anycpu, dsdt_anycpu_len);
         nr_processor_objects = HVM_MAX_VCPUS;
     }
+    update_dsdt(dsdt);
 
     /*
      * N.B. ACPI 1.0 operating systems may not handle FADT with revision 2
--- a/tools/firmware/hvmloader/acpi/static_tables.c
+++ b/tools/firmware/hvmloader/acpi/static_tables.c
@@ -69,7 +69,7 @@
     .iapc_boot_arch = ACPI_8042,
     .flags = (ACPI_PROC_C1 |
               ACPI_WBINVD |
-              ACPI_FIX_RTC | ACPI_TMR_VAL_EXT |
+              ACPI_RTC_S4 | ACPI_TMR_VAL_EXT |
               ACPI_USE_PLATFORM_CLOCK),
 
     .reset_reg = {
--- a/tools/firmware/hvmloader/util.c
+++ b/tools/firmware/hvmloader/util.c
@@ -719,6 +719,19 @@
     return ((hpet_id >> 16) == 0x8086);
 }
 
+int get_s3_enabled(void)
+{
+    struct hvm_info_table *t = get_hvm_info_table();
+    return (t ? t->s3_enabled : 0);
+}
+
+int get_s4_enabled(void)
+{
+    struct hvm_info_table *t = get_hvm_info_table();
+    return (t ? t->s4_enabled : 0);
+}
+
+
 /*
  * Local variables:
  * mode: C
--- a/tools/firmware/hvmloader/util.h
+++ b/tools/firmware/hvmloader/util.h
@@ -141,6 +141,10 @@
 struct hvm_info_table *get_hvm_info_table(void) __attribute__ ((const));
 #define hvm_info (get_hvm_info_table())
 
+/* ACPI sleep support available for this guest? */
+int get_s3_enabled(void);
+int get_s4_enabled(void);
+
 /* String and memory functions */
 int strcmp(const char *cs, const char *ct);
 int strncmp(const char *s1, const char *s2, uint32_t n);
--- a/xen/include/public/hvm/hvm_info_table.h
+++ b/xen/include/public/hvm/hvm_info_table.h
@@ -70,6 +70,10 @@
 
     /* Bitmap of which CPUs are online at boot time. */
     uint8_t     vcpu_online[(HVM_MAX_VCPUS + 7)/8];
+
+    /* Do we support ACPI S3 and S4 states? */
+    uint8_t     s3_enabled;    
+    uint8_t     s4_enabled;    
 };
 
 #endif /* __XEN_PUBLIC_HVM_HVM_INFO_TABLE_H__ */
